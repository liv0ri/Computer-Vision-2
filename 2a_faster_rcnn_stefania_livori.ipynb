{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2a: Object Detection with Faster R-CNN\n",
                "\n",
                "**Student:** Stefania Livori  \n",
                "**Model:** Faster R-CNN (ResNet50 FPN)\n",
                "\n",
                "This notebook implements a Faster R-CNN object detector to detect Maltese traffic signs. It covers:\n",
                "1.  **Dataset Preparation**: Loading images and annotations.\n",
                "2.  **Model Training**: Fine-tuning a pre-trained Faster R-CNN model.\n",
                "3.  **Evaluation**: Calculating F1-Scores.\n",
                "4.  **Inference & Analytics**: visualizing detections and counting signs per image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df0464cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "import torchvision.transforms as T\n",
                "from PIL import Image\n",
                "import json, os\n",
                "import matplotlib.pyplot as plt\n",
                "from stefania_livori_utils import *\n",
                "\n",
                "# Ensure reproducible results\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9bab7270",
            "metadata": {},
            "source": [
                "## 1. Dataset Preparation\n",
                "\n",
                "I define the `SignsDataset` class to load images and COCO-style annotations. \n",
                "**Note:** Images are stored in `data/` and annotations in `json_stefania.json`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd3839a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Update paths here to match your environment\n",
                "DATA_DIR = \"label-studio/label-studio/media/upload/2\" \n",
                "ANNOTATION_FILE = \"stefania_livori/result.json\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "caf93971",
            "metadata": {},
            "outputs": [],
            "source": [
                "CLASS_NAME_TO_ID = {\n",
                "    \"Background\": 0,\n",
                "    \"Stop\": 1,\n",
                "    \"No Entry (One Way)\": 2,\n",
                "    \"Pedestrian Crossing\": 3,\n",
                "    \"Roundabout Ahead\": 4,\n",
                "    \"No Through Road (T-Sign)\": 5,\n",
                "    \"Blind-Spot Mirror (Convex)\": 6\n",
                "}\n",
                "\n",
                "# Reverse mapping for visualization\n",
                "CLASS_NAMES = {v: k for k, v in CLASS_NAME_TO_ID.items()}\n",
                "\n",
                "# +1 for background\n",
                "NUM_CLASSES = len(CLASS_NAME_TO_ID) + 1  \n",
                "\n",
                "# https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96\n",
                "class SignsDataset(Dataset):\n",
                "    def __init__(self, root, annFile, transforms=None, preload=True):\n",
                "        self.root = root\n",
                "        self.transforms = transforms\n",
                "\n",
                "        # Load the JSON file\n",
                "        with open(annFile) as f:\n",
                "            data = json.load(f)\n",
                "\n",
                "        # Extract images and annotations\n",
                "        self.images_info = data[\"images\"]\n",
                "        self.annotations = data[\"annotations\"]\n",
                "\n",
                "        # Map image_id to annotations for faster access\n",
                "        self.imgToAnns = {img[\"id\"]: [] for img in self.images_info}\n",
                "        for ann in self.annotations:\n",
                "            self.imgToAnns[ann[\"image_id\"]].append(ann)\n",
                "\n",
                "        self.preload = preload\n",
                "        # If preload is True, load all images into memory\n",
                "        if preload:\n",
                "            # Preload the images into memory for speed\n",
                "            # This might cause memory usage\n",
                "            self.loaded_images = []\n",
                "            for img_info in self.images_info:\n",
                "                # Handle potential path differences if filename contains folders\n",
                "                # Assuming images are directly in root or filename matches relative structure\n",
                "                img_name = os.path.basename(img_info[\"file_name\"])\n",
                "                img_path = os.path.join(root, img_name)\n",
                "                \n",
                "                try:\n",
                "                    with Image.open(img_path) as img:\n",
                "                        self.loaded_images.append(img.convert(\"RGB\").copy())\n",
                "                except FileNotFoundError:\n",
                "                    # Error if an image is not found\n",
                "                    raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_info = self.images_info[idx]\n",
                "        image_id = img_info[\"id\"]\n",
                "\n",
                "        if self.preload:\n",
                "            # load the image \n",
                "            img = self.loaded_images[idx].copy()\n",
                "        else:\n",
                "            img_name = os.path.basename(img_info[\"file_name\"])\n",
                "            img_path = os.path.join(self.root, img_name)\n",
                "            img = Image.open(img_path).convert(\"RGB\")\n",
                "\n",
                "        anns = self.imgToAnns[image_id]\n",
                "        boxes = []\n",
                "        labels = []\n",
                "\n",
                "        for ann in anns:\n",
                "            x, y, w, h = ann[\"bbox\"]\n",
                "            # Based on the coco definition\n",
                "            boxes.append([x, y, x + w, y + h])\n",
                "            labels.append(ann[\"category_id\"]+1)\n",
                "\n",
                "        # Guard against images with no annotations if necessary\n",
                "        # if not boxes:\n",
                "        #      # Handle empty target\n",
                "        #      boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
                "        #      labels = torch.zeros((0,), dtype=torch.int64)\n",
                "        # else:\n",
                "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
                "        labels = torch.tensor(labels, dtype=torch.int64)\n",
                "\n",
                "        target = {\n",
                "            \"boxes\": boxes,\n",
                "            \"labels\": labels,\n",
                "            # Converted to pytorch tensor\n",
                "            \"image_id\": torch.tensor([image_id])\n",
                "        }\n",
                "\n",
                "        if self.transforms:\n",
                "            img = self.transforms(img)\n",
                "\n",
                "        # For debugging: print target labels in torch tensor format\n",
                "        # print(torch.unique(target[\"labels\"]))\n",
                "\n",
                "        return img, target\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.images_info)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a44c8a05",
            "metadata": {},
            "source": [
                "### Initialize Dataset and DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a410874",
            "metadata": {},
            "outputs": [],
            "source": [
                "# No data augmentation for now\n",
                "transform = T.Compose([\n",
                "    T.ToTensor(),\n",
                "])\n",
                "\n",
                "dataset = SignsDataset(\n",
                "    root=DATA_DIR,\n",
                "    annFile=ANNOTATION_FILE,\n",
                "    transforms=transform\n",
                ")\n",
                "\n",
                "# Split dataset into training and validation sets\n",
                "val_size = int(0.2 * len(dataset))\n",
                "train_size = len(dataset) - val_size\n",
                "# Partitions the dataset into training and validation sets once and keeps as it is to have the validation set fixed and unseen\n",
                "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
                "\n",
                "print(f\"Training set size: {len(train_dataset)}\")\n",
                "print(f\"Validation set size: {len(val_dataset)}\")\n",
                "\n",
                "# https://github.com/GirinChutia/FasterRCNN-Torchvision-FineTuning/blob/main/train.py\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=4,\n",
                "    # shuffle the dataset\n",
                "    shuffle=True,\n",
                "    # Batch collate function to handle variable-size images\n",
                "    collate_fn=lambda x: tuple(zip(*x))\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=2,\n",
                "    shuffle=False,\n",
                "    # Batch collate function to handle variable-size images\n",
                "    collate_fn=lambda x: tuple(zip(*x))\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "821b57e4",
            "metadata": {},
            "source": [
                "## 2. Model Configuration\n",
                "Loading the Faster R-CNN model pre-trained on COCO and adapting proper number of classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7acfd4a",
            "metadata": {},
            "outputs": [],
            "source": [
                "device = get_device()\n",
                "model = get_faster_rcnn(NUM_CLASSES).to(device)\n",
                "\n",
                "# Optimizer & Scheduler\n",
                "# https://github.com/GirinChutia/FasterRCNN-Torchvision-FineTuning/blob/main/train.py\n",
                "# optimizer logic based on the github link above\n",
                "optimizer = torch.optim.SGD(\n",
                "    model.parameters(),\n",
                "    lr=0.005,\n",
                "    # Momentum included to speed up training\n",
                "    momentum=0.9,\n",
                "    weight_decay=0.0005\n",
                ")\n",
                "\n",
                "\n",
                "# Might need to try different step sizes\n",
                "scheduler = torch.optim.lr_scheduler.StepLR(\n",
                "    optimizer, step_size=3, gamma=0.1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "76cbb208",
            "metadata": {},
            "source": [
                "## 3. Training Loop\n",
                "Training for 25 epochs and evaluating on validation set after each epoch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a964cb29",
            "metadata": {},
            "outputs": [],
            "source": [
                "# number of epochs - needs to be set to 25\n",
                "num_epochs = 1\n",
                "\n",
                "# Training Loop\n",
                "# https://medium.com/@RobuRishabh/understanding-and-implementing-faster-r-cnn-248f7b25ff96 \n",
                "for epoch in range(num_epochs):\n",
                "    print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}\")\n",
                "    avg_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
                "    scheduler.step()\n",
                "    \n",
                "    # Evaluate\n",
                "    f1 = f1_score_by_iou(model, val_loader, device)\n",
                "\n",
                "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} | f1-score: {f1:.4f}\")\n",
                "\n",
                "print(\"Training finished!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2dd9eaaf",
            "metadata": {},
            "source": [
                "## 4. Analytics & Visualization\n",
                "\n",
                "Analyze the detections on the validation set. I will count the number of detected signs and visualize the results. I apply inference on the validation set which are unlabelled unseen examples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f38538c",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "\n",
                "# Analytic - Count detections per image in validation set\n",
                "# print(\"GT labels:\", target[\"labels\"])\n",
                "# print(\"Pred labels:\", pred[\"labels\"][:5])\n",
                "# print(\"Scores:\", pred[\"scores\"][:5])\n",
                "total_detected_signs = 0\n",
                "results_summary = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for i, (img, target) in enumerate(val_dataset):\n",
                "        # Inference\n",
                "        prediction = model([img.to(device)])[0]\n",
                "        \n",
                "        # Evaluate based on the threshold\n",
                "        keep = prediction[\"scores\"] > 0.3\n",
                "        final_boxes = prediction[\"boxes\"][keep]\n",
                "        final_labels = prediction[\"labels\"][keep]\n",
                "        final_scores = prediction[\"scores\"][keep]\n",
                "        \n",
                "        count = len(final_labels)\n",
                "        total_detected_signs += count\n",
                "        \n",
                "        results_summary.append({\n",
                "            \"ImageID\": target[\"image_id\"].item(),\n",
                "            \"DetectedSigns\": count,\n",
                "            \"Labels\": [CLASS_NAMES.get(l.item(), \"Unknown\") for l in final_labels]\n",
                "        })\n",
                "\n",
                "print(f\"Total signs detected in validation set: {total_detected_signs}\")\n",
                "print(\"Results summary:\")\n",
                "for res in results_summary:\n",
                "    print(res)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95add3c7",
            "metadata": {},
            "source": [
                "### Save the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f6eaf91",
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save(model.state_dict(), \"./faster_rcnn_stefania_livori.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b2e1b13",
            "metadata": {},
            "source": [
                "### Visualisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53d6f112",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nVisualizing Sample Predictions:\")\n",
                "for i in range(len(val_dataset)):\n",
                "    img, target = val_dataset[i]\n",
                "    with torch.no_grad():\n",
                "        prediction = model([img.to(device)])[0]\n",
                "\n",
                "    print(f\"Image {i+1} Ground Truth: {[CLASS_NAMES[l.item()] for l in target['labels']]} \")\n",
                "    visualize_predictions(img, prediction, CLASS_NAMES, threshold=0.3)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ari50",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
