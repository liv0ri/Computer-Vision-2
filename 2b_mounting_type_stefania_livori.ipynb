{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2b: Attribute Classification (Mounting Type)\n",
                "\n",
                "**Student:** Stefania Livori  \n",
                "**Attribute:** Mounting Type (Pole-mounted vs Wall-mounted)  \n",
                "**Model:** Faster R-CNN (ResNet50 FPN)\n",
                "\n",
                "This notebook implements a detector specifically for classifying the mounting type of traffic signs. It covers:\n",
                "1.  **Dataset Preparation**: Custom `MountingDataset` parsing \"mounting\" attributes.\n",
                "2.  **Model Training**: Training to detect and classify 'Pole-mounted' vs 'Wall-mounted'.\n",
                "3.  **Evaluation**: Calculating Mean Average Precision (mAP).\n",
                "4.  **Inference & Analytics**: Counting the distribution of mounting types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "import torchvision.transforms as T\n",
                "from PIL import Image\n",
                "import json\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "from stefania_livori_utils import *\n",
                "\n",
                "# Ensure reproducible results\n",
                "torch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "31c867b3",
            "metadata": {},
            "source": [
                "## 1. Dataset Preparation\n",
                "\n",
                "We define the `MountingDataset` class. This class parses the Label Studio JSON export to extract the \"mounting\" attribute for each sign.\n",
                "**Robustness Update:** This version correctly handles images with multiple signs having different mounting types by checking for linked IDs (`from_id`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13bd31eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mounting Classes mapping\n",
                "MOUNTING_CLASSES = {\n",
                "    1: \"Pole-mounted\",\n",
                "    2: \"Wall-mounted\"\n",
                "}\n",
                "\n",
                "NUM_CLASSES = len(MOUNTING_CLASSES) + 1 # +1 for background\n",
                "\n",
                "class MountingDataset(Dataset):\n",
                "    # root - folder containing images\n",
                "    # ann_file - Label Studio JSON export\n",
                "    # transforms - image preprocessing\n",
                "    # preload - load images into memory for faster training\n",
                "    def __init__(self, root, ann_file, transforms=None, preload=True):\n",
                "        self.root = root\n",
                "        self.transforms = transforms\n",
                "\n",
                "        # Load the annotations\n",
                "        with open(ann_file) as f:\n",
                "            self.tasks = json.load(f)\n",
                "\n",
                "        self.preload = preload\n",
                "        # Preload images into memory for faster training\n",
                "        if preload:\n",
                "            self.loaded_images = []\n",
                "            for i, task in enumerate(self.tasks):\n",
                "                if \"data\" in task and \"image\" in task[\"data\"]:\n",
                "                    img_rel_path = task[\"data\"][\"image\"].replace(\"/data/upload/\", \"\")\n",
                "                    img_path = os.path.join(self.root, img_rel_path)\n",
                "                elif \"file_name\" in task:\n",
                "                     img_path = os.path.join(self.root, task[\"file_name\"])\n",
                "                else:\n",
                "                     self.loaded_images.append(None)\n",
                "                     continue\n",
                "\n",
                "                try:\n",
                "                    with Image.open(img_path) as img:\n",
                "                        self.loaded_images.append(img.convert(\"RGB\").copy())\n",
                "                except FileNotFoundError:\n",
                "                    raise FileNotFoundError(f\"Warning: Image not found {img_path}\")\n",
                "\n",
                "        self.map = {\n",
                "            \"Pole-mounted\": 1,\n",
                "            \"Wall-mounted\": 2\n",
                "        }\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.tasks)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        task = self.tasks[idx]\n",
                "\n",
                "        if self.preload:\n",
                "            img = self.loaded_images[idx]\n",
                "            # Handle missing images by skipping\n",
                "            if img is None: \n",
                "                raise FileNotFoundError(f\"Image not found at index {idx}\")\n",
                "            img = img.copy()\n",
                "        else:\n",
                "            if \"data\" in task and \"image\" in task[\"data\"]:\n",
                "                img_rel_path = task[\"data\"][\"image\"].replace(\"/data/upload/\", \"\")\n",
                "                img_path = os.path.join(self.root, img_rel_path)\n",
                "            else:\n",
                "                 img_path = \"\"\n",
                "            img = Image.open(img_path).convert(\"RGB\")\n",
                "\n",
                "        # Load the bounding boxes and labels\n",
                "        boxes, labels = [], []\n",
                "\n",
                "        if \"annotations\" in task:\n",
                "            for ann in task[\"annotations\"]:\n",
                "                # Robust parsing for multiple objects/attributes\n",
                "                # id -> rect_info\n",
                "                rect_map = {}    \n",
                "                # rect_id -> mount_class\n",
                "                mount_map = {}    \n",
                "                global_mount = None\n",
                "\n",
                "                if \"result\" in ann:\n",
                "                    # Pass 1: Gather all regions and choices\n",
                "                    for r in ann[\"result\"]:\n",
                "                        rid = r.get(\"id\")\n",
                "                        \n",
                "                        if r[\"type\"] == \"rectanglelabels\":\n",
                "                            rect_map[rid] = r\n",
                "                        \n",
                "                        elif r[\"from_name\"] == \"mounting\":\n",
                "                            if \"value\" in r and \"choices\" in r[\"value\"] and len(r[\"value\"][\"choices\"]) > 0:\n",
                "                                val = r[\"value\"][\"choices\"][0]\n",
                "                                mount_label = self.map.get(val, None)\n",
                "                                \n",
                "                                if mount_label is not None:\n",
                "                                    # Check linkage via from_id\n",
                "                                    if \"from_id\" in r:\n",
                "                                        mount_map[r[\"from_id\"]] = mount_label\n",
                "                                    else:\n",
                "                                        # Fallback to global if not linked\n",
                "                                        global_mount = mount_label\n",
                "                \n",
                "                    # Pass 2: Generate prediction targets\n",
                "                    iw, ih = img.size\n",
                "                    for rid, r in rect_map.items():\n",
                "                        # Priority: Specific Link > Global Fallback\n",
                "                        m = mount_map.get(rid, global_mount)\n",
                "                        \n",
                "                        if m is not None:\n",
                "                            x = r[\"value\"][\"x\"]\n",
                "                            y = r[\"value\"][\"y\"]\n",
                "                            w = r[\"value\"][\"width\"]\n",
                "                            h = r[\"value\"][\"height\"]\n",
                "                            \n",
                "                            boxes.append([\n",
                "                                x/100*iw, y/100*ih,\n",
                "                                (x+w)/100*iw, (y+h)/100*ih\n",
                "                            ])\n",
                "                            labels.append(m)\n",
                "\n",
                "        if not boxes:\n",
                "             boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
                "             labels = torch.zeros((0,), dtype=torch.int64)\n",
                "        else:\n",
                "             boxes = torch.tensor(boxes, dtype=torch.float32)\n",
                "             labels = torch.tensor(labels, dtype=torch.int64)\n",
                "\n",
                "        target = {\n",
                "            \"boxes\": boxes,\n",
                "            \"labels\": labels,\n",
                "            \"image_id\": torch.tensor([idx])\n",
                "        }\n",
                "\n",
                "        if self.transforms:\n",
                "            img = self.transforms(img)\n",
                "\n",
                "        return img, target"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0cb273f4",
            "metadata": {},
            "source": [
                "### Initialize Dataset and DataLoaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ffe8577e",
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = T.Compose([T.ToTensor()])\n",
                "\n",
                "# Update paths\n",
                "DATA_DIR = \"label-studio/label-studio/media/upload\"\n",
                "ANNOTATION_FILE = \"json_stefania.json\"\n",
                "\n",
                "dataset = MountingDataset(\n",
                "    root=DATA_DIR,\n",
                "    ann_file=ANNOTATION_FILE,\n",
                "    transforms=transform\n",
                ")\n",
                "\n",
                "# Filter\n",
                "if dataset.preload and any(img is None for img in dataset.loaded_images):\n",
                "    print(\"Warning: Some images failed to load.\")\n",
                "\n",
                "# Split dataset\n",
                "total_size = len(dataset)\n",
                "val_size = int(0.2 * total_size)\n",
                "train_size = total_size - val_size\n",
                "\n",
                "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
                "\n",
                "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
                "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
                "\n",
                "print(f\"Train size: {len(train_ds)}, Val size: {len(val_ds)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b2b1367d",
            "metadata": {},
            "source": [
                "## 2. Model Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9421fe2",
            "metadata": {},
            "outputs": [],
            "source": [
                "device = get_device()\n",
                "model = get_faster_rcnn(NUM_CLASSES).to(device)\n",
                "\n",
                "optimizer = torch.optim.SGD(\n",
                "    model.parameters(),\n",
                "    lr=0.005,\n",
                "    momentum=0.9,\n",
                "    weight_decay=0.0005\n",
                ")\n",
                "\n",
                "# Might need to try different step sizes\n",
                "scheduler = torch.optim.lr_scheduler.StepLR(\n",
                "    optimizer, step_size=3, gamma=0.1\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "615f803c",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e007e92",
            "metadata": {},
            "outputs": [],
            "source": [
                "num_epochs = 3\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    print(f\"\\nStarting Epoch {epoch+1}/{num_epochs}\")\n",
                "    loss = train_one_epoch(model, train_loader, optimizer, device)\n",
                "    scheduler.step()\n",
                "    f1 = f1_score_by_iou(model, val_loader, device)\n",
                "    print(f\"Epoch {epoch+1} | Loss {loss:.4f} | f1-score {f1:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96e6bf52",
            "metadata": {},
            "source": [
                "## 4. Analytics & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2598014",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "\n",
                "class_counts = {name: 0 for name in MOUNTING_CLASSES.values()}\n",
                "\n",
                "with torch.no_grad():\n",
                "    for img, target in val_ds:\n",
                "        prediction = model([img.to(device)])[0]\n",
                "        \n",
                "        keep = prediction[\"scores\"] > 0.5\n",
                "        labels = prediction[\"labels\"][keep]\n",
                "        \n",
                "        for l in labels:\n",
                "            name = MOUNTING_CLASSES.get(l.item(), \"Unknown\")\n",
                "            if name in class_counts:\n",
                "                class_counts[name] += 1\n",
                "\n",
                "print(\"Detected Mounting Types Distribution:\")\n",
                "for name, count in class_counts.items():\n",
                "    print(f\"{name}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0bac5989",
            "metadata": {},
            "source": [
                "### Visualisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2457308",
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.save(model.state_dict(), \"d:/Uni/Computer-Vision-2/stefania_livori_mounting.pt\")\n",
                "print(\"\\nVisualizing Sample Predictions:\")\n",
                "for i in range(len(val_ds)):\n",
                "    img, target = val_ds[i]\n",
                "    with torch.no_grad():\n",
                "        prediction = model([img.to(device)])[0]\n",
                "    \n",
                "    gt_labels = [MOUNTING_CLASSES.get(l.item(), \"Unknown\") for l in target['labels']]\n",
                "    print(f\"Sample {i+1} GT: {gt_labels}\")\n",
                "    visualize_predictions(img, prediction, MOUNTING_CLASSES, threshold=0.5)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ari50",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
