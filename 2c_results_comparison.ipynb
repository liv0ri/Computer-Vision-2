{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f413ff3",
   "metadata": {},
   "source": [
    "# Results comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4fbc6",
   "metadata": {},
   "source": [
    "# Task 3: Results comparison\n",
    "\n",
    "This notebook implements an evaluation to check whether the model is able to detect Maltese traffic signs. It covers:\n",
    "1. **Evaluation**: Calculating F1-Scores & recalls.\n",
    "2. **Inference & Analytics**: visualizing detections and counting signs per image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87897901",
   "metadata": {},
   "source": [
    "## Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf673989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready: all dependencies installed and imported.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "#  Standard library \n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "#  Package bootstrap helper\n",
    "def ensure_package(pkg: str, import_name: str | None = None, pip_name: str | None = None):\n",
    "    try:\n",
    "        return importlib.import_module(import_name or pkg)\n",
    "    except ImportError:\n",
    "        pip_target = pip_name or pkg\n",
    "        print(f\"Installing missing package: {pip_target}\")\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", pip_target],\n",
    "            stdout=subprocess.DEVNULL\n",
    "        )\n",
    "        return importlib.import_module(import_name or pkg)\n",
    "\n",
    "# Ensure third-party packages \n",
    "torch = ensure_package(\"torch\")\n",
    "np = ensure_package(\"numpy\")\n",
    "torchvision = ensure_package(\"torchvision\")\n",
    "ensure_package(\"pycocotools\")\n",
    "\n",
    "# Explicit imports (clear & IDE-friendly) \n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Custom/local utilities \n",
    "try:\n",
    "    from stefania_livori_utils import *\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"‚ùå 'stefania_livori_utils' not found.\\n\"\n",
    "        \"Make sure 'stefania_livori_utils.py' is in the working directory \"\n",
    "        \"or on PYTHONPATH.\"\n",
    "    )\n",
    "\n",
    "#  Reproducibility \n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Environment ready: all dependencies installed and imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e804b2",
   "metadata": {},
   "source": [
    "## Declare the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51fcae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c59b3",
   "metadata": {},
   "source": [
    "## Variable to hold the type of evaluation we are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a13865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = \"sign\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc69886",
   "metadata": {},
   "source": [
    "## Dataset if using signs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval == \"sign\":\n",
    "    ANNOTATION_FILE_VAL = \"Assignment Material/COCO-based_COCO/annotations/val.json\"\n",
    "    DATA_DIR_VAL = \"Assignment Material/COCO-based_COCO/images/val\"\n",
    "    ANNOTATION_FILE_TEST = \"Assignment Material/COCO-based_COCO/annotations/test.json\"\n",
    "    DATA_DIR_TEST = \"Assignment Material/COCO-based_COCO/images/test\"\n",
    "    with open(ANNOTATION_FILE_VAL, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    CLASS_ID_TO_NAME = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
    "    NUM_CLASSES = len(CLASS_ID_TO_NAME) + 1     \n",
    "    model = get_faster_rcnn(NUM_CLASSES)\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"./models/faster_rcnn_stefania_livori.pt\", map_location=device)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779a476",
   "metadata": {},
   "source": [
    "## Dataset if using mounting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if eval == \"mounting\":\n",
    "    DATA_DIR_VAL = \"Assignment Material/COCO-based_COCO_mounting/images/val\"\n",
    "    ANNOTATION_FILE_VAL = \"Assignment Material/COCO-based_COCO_mounting/annotations/val.json\"\n",
    "    ANNOTATION_FILE_TEST = \"Assignment Material/COCO-based_COCO_mounting/annotations/test.json\"\n",
    "    DATA_DIR_TEST = \"Assignment Material/COCO-based_COCO_mounting/images/test\"\n",
    "    model = get_retinanet(NUM_CLASSES)\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"./models/mounting_retinanet_stefania_livori.pt\", map_location=device)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15556968",
   "metadata": {},
   "source": [
    "## COCO-based Only MAP Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaff9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_map(model, data_loader, device, coco_gt):\n",
    "    model.eval()\n",
    "    coco_results = []\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for target, output in zip(targets, outputs):\n",
    "            image_id = int(target[\"image_id\"])\n",
    "            boxes = output[\"boxes\"].cpu().numpy()\n",
    "            scores = output[\"scores\"].cpu().numpy()\n",
    "            labels = output[\"labels\"].cpu().numpy()\n",
    "\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                coco_results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(label),\n",
    "                    \"bbox\": [\n",
    "                        float(box[0]),\n",
    "                        float(box[1]),\n",
    "                        float(box[2] - box[0]),\n",
    "                        float(box[3] - box[1]),\n",
    "                    ],\n",
    "                    \"score\": float(score),\n",
    "                })\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval.stats  # contains mAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bf14e",
   "metadata": {},
   "source": [
    "Run the actual evaluation for COCO based models only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949c55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.160\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.060\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.199\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.254\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.266\n",
      "mAP@0.5:0.95: 0.05896392620075338\n",
      "mAP@0.5: 0.1604820357999089\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "coco_gt = COCO(ANNOTATION_FILE_VAL)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "val_dataset = SignsDataset(\n",
    "    root=DATA_DIR_VAL,\n",
    "    annFile=ANNOTATION_FILE_VAL,\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    # Batch collate function to handle variable-size images\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "map_stats = evaluate_map(model, val_loader, device, coco_gt)\n",
    "\n",
    "print(\"mAP@0.5:0.95:\", map_stats[0])\n",
    "print(\"mAP@0.5:\", map_stats[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ari50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
